<!DOCTYPE html>
<!--[if IE 6]>
<html id="ie6" lang="en-US">
<![endif]-->
<!--[if IE 7]>
<html id="ie7" lang="en-US">
<![endif]-->
<!--[if IE 8]>
<html id="ie8" lang="en-US">
<![endif]-->
<!--[if !(IE 6) | !(IE 7) | !(IE 8)  ]><!-->
<html lang="en-US">
<!--<![endif]-->
<head>
<meta name="google-site-verification" content="C2jGLB_j53XmXMPfzladiCFXTMOKg_RHJ_OBAQgfOrg" />
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width" />

<title>Installing and using Scrapy web crawler to search text on multiple sites</title>
<link rel="profile" href="//gmpg.org/xfn/11" />
<link rel="stylesheet" type="text/css" media="all" href="//opensourcehacker.com/wp-content/themes/twentyeleven/style.css" />

<link rel="stylesheet" type="text/css" media="all" href="//opensourcehacker.com/wp-content/themes/twentyeleven/google-code-prettify/prettify.css" />
<script src="//opensourcehacker.com/wp-content/themes/twentyeleven/google-code-prettify/prettify.js" type="text/javascript"></script>
<script>
      document.addEventListener("DOMContentLoaded", prettyPrint);               
</script>

<script
  src   = "http://cdn.webglstats.com/stat.js"
  defer = "defer"
  async = "async"
></script>

<link rel="pingback" href="//opensourcehacker.com/xmlrpc.php" />
<!--[if lt IE 9]>
<script src="//opensourcehacker.com/wp-content/themes/twentyeleven/js/html5.js" type="text/javascript"></script>
<![endif]-->
<!--[if IE 7]><link rel='stylesheet' id='css-ie-fix' href='//opensourcehacker.com/wp-content/plugins/special-recent-posts/css/css-ie7-fix.css' type='text/css' media='all' /> <![endif]--><link rel='stylesheet' id='srp-front-stylesheet-css'  href='//opensourcehacker.com/wp-content/plugins/special-recent-posts/css/css-front.css?ver=4.1.1' type='text/css' media='all' />
<link rel='stylesheet' id='sedlex_styles-css'  href='//opensourcehacker.com/wp-content/plugins/content-table/core/load-styles.php?c=0&#038;load=7e487b67b8e32886e4d732079da67f5b&#038;ver=20150326' type='text/css' media='all' />
<link rel='shortlink' href='//opensourcehacker.com/?p=1073' />

<!-- All in One SEO Pack 1.6.13.8 by Michael Torbert of Semper Fi Web Design[469,569] -->
<meta name="keywords" content="crawl,find,follow,full text,full text search,pdf,pypdf,python,scrape,scrapy,search,trademark,user policy,violation,web crawler" />
<link rel="canonical" href="https://opensourcehacker.com/2011/03/08/installing-and-using-scrapy-web-crawler-to-search-text-on-multiple-sites/" />
<!-- /all in one seo pack -->
<link rel='stylesheet' id='wop-css'  href='//opensourcehacker.com/wp-content/plugins/widgets-on-pages/wop.css' type='text/css' media='all' /><meta property="fb:admins" content="611693032"/><meta property="og:title" content="Installing and using Scrapy web crawler to search text on multiple sites"/><meta property="og:type" content="article"/><meta property="og:url" content="https://opensourcehacker.com/2011/03/08/installing-and-using-scrapy-web-crawler-to-search-text-on-multiple-sites/"/><meta property="og:site_name" content="Open Source Hacker"/><meta property="og:image" content="http://opensourcehacker.com/wp-content/uploads/2011/08/icon.png"/><meta name="twitter:card" content="summary">
<meta name="twitter:url" content="https://opensourcehacker.com/2011/03/08/installing-and-using-scrapy-web-crawler-to-search-text-on-multiple-sites/">
<meta name="twitter:title" content="Installing and using Scrapy web crawler to search text on multiple sites">
<meta name="twitter:description" content="Here is a little script to use Scrapy, a web crawling framework for Python, to search sites for references for certain texts including link content and PDFs. This is handy for cases where you need to find links violating the user policy,  trademarks which are not allowed or just to see where your template output is being used.  Our Scrapy example differs from a normal search engine as it does HTML source code level checking:">
</head>

<body class="single single-post postid-1073 single-format-standard single-author singular two-column right-sidebar">
<div id="page" class="hfeed">
	<header id="branding" role="banner">
			<hgroup>
				<h1 id="site-title"><span><a href="https://opensourcehacker.com/" title="Open Source Hacker" rel="home">Open Source Hacker</a></span></h1>
				<h2 id="site-description">Pushing the boundaries of free technology</h2>

			</hgroup>

						<a href="http://opensourcehacker.com/">
									<img src="//opensourcehacker.com/wp-content/uploads/2013/06/Untitled-4.jpg" width="1000" height="288" alt="" />
							</a>
			
			                                				<iframe id="ad" allowtransparency="true" frameborder="0" hspace="0" vspace="0" marginheight="0" marginwidth="0" scrolling="no" width="234" height="60" src="https://localbitcoins.com/affiliate-embed/half-banner?ref=1af"></iframe>

				<!-- get_search_form() -->
 			
			<nav id="access" role="navigation">
				<h3 class="assistive-text">Main menu</h3>
								<div class="skip-link"><a class="assistive-text" href="#content" title="Skip to primary content">Skip to primary content</a></div>
				<div class="skip-link"><a class="assistive-text" href="#secondary" title="Skip to secondary content">Skip to secondary content</a></div>
								<div class="menu"><ul><li ><a href="https://opensourcehacker.com/">Home</a></li><li class="page_item page-item-1491"><a href="https://opensourcehacker.com/archives/" title="Archives">Archives</a></li><li class="page_item page-item-1433"><a href="https://opensourcehacker.com/contact/" title="Contact">Contact</a></li><li class="page_item page-item-1437"><a href="https://opensourcehacker.com/references-cv-resume/" title="References">References</a></li></ul></div>

                                <a href="https://twitter.com/moo9000" class="twitter-follow-button" data-button="grey" data-width="100" data-show-count="false" data-link-color="ffFFff" data-text-color="ffFFff" >Follow @moo9000</a>
				<script src="//platform.twitter.com/widgets.js" type="text/javascript"></script> 


<iframe id="facebook" src="https://www.facebook.com/plugins/like.php?app_id=201995916528656&amp;href=http%3A%2F%2Fwww.facebook.com%2Fpages%2FOpen-Source-Hacker%2F181710458567630&amp;send=false&amp;layout=button_count&amp;width=200&amp;show_faces=true&amp;action=like&amp;colorscheme=dark&amp;font=trebuchet+ms&amp;height=21" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:100px; height:21px;" allowTransparency="true"></iframe>                                


<!-- Place this tag where you want the +1 button to render -->
<div id="plus">
<div class="g-plusone" data-size="small" data-href="http://opensourcehacker.com"></div>
</div>

<!-- Place this render call where appropriate -->
<script type="text/javascript">
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>
			</nav><!-- #access -->
	</header><!-- #branding -->


	<div id="main">
		<div id="primary">
			<div id="content" role="main">

				
					<nav id="nav-single">
						<h3 class="assistive-text">Post navigation</h3>
						<span class="nav-previous"><a href="https://opensourcehacker.com/2011/03/04/sauna-sprint-2011-%e3%82%b5%e3%83%b3%e3%82%bf%e3%82%b9%e3%83%97%e3%83%aa%e3%83%b3%e3%83%88/" rel="prev"><span class="meta-nav">&larr;</span> Previous</a></span>
						<span class="nav-next"><a href="https://opensourcehacker.com/2011/03/09/lazily-load-elements-becoming-visible-using-jquery/" rel="next">Next <span class="meta-nav">&rarr;</span></a></span>
					</nav><!-- #nav-single -->

					
<article id="post-1073" class="post-1073 post type-post status-publish format-standard hentry category-python category-technology tag-crawl tag-find tag-follow tag-full-text tag-full-text-search tag-pdf tag-pypdf tag-python tag-scrape tag-scrapy tag-search tag-trademark tag-user-policy tag-violation tag-web-crawler">
	<header class="entry-header">
		<h1 class="entry-title">Installing and using Scrapy web crawler to search text on multiple sites</h1>

				<div class="entry-meta">
			<span class="sep">Posted on </span><a href="https://opensourcehacker.com/2011/03/08/installing-and-using-scrapy-web-crawler-to-search-text-on-multiple-sites/" title="16:37" rel="bookmark"><time class="entry-date" datetime="2011-03-08T16:37:32+00:00" pubdate>2011-03-08</time></a><span class="by-author"> <span class="sep"> by </span> <span class="author vcard"><a class="url fn n" href="https://opensourcehacker.com/author/moo/" title="View all posts by Mikko Ohtamaa" rel="author">Mikko Ohtamaa</a></span></span>		</div><!-- .entry-meta -->
			</header><!-- .entry-header -->

	<div class="entry-content">
		<p>Here is a little script to use <a href="http://scrapy.org/">Scrapy</a>, a web crawling framework for Python, to search sites for references for certain texts including link content and PDFs. This is handy for cases where you need to find links violating the user policy,  trademarks which are not allowed or just to see where your template output is being used.  Our Scrapy example differs from a normal search engine as it does HTML source code level checking: you can also search for CSS classes, link targets and other elements which may be invisible for normal search engines.</p>
<p>Scrapy comes with a command-line tool and project skeleton generator. You need to generate your own Scrapy project to where you can then add your own spider classes.</p>
<p>Install Scrapy using Distribute (or setuptools):</p>
<pre>easy_install Scrapy</pre>
<p>Create project code skeleton:</p>
<pre>scrapy startproject myscraper</pre>
<p>Add your spider class skeleton by creating a file <em>myscraper/spiders/spiders.py</em>:</p>
<pre>from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor

class MySpider(CrawlSpider):
 """ Crawl through web sites you specify """

 name = "mycrawler"

 # Stay within these domains when crawling
 allowed_domains = ["www.mysite.com"]

 start_urls = [
 "http://www.mysite.com/",
 ]

 # Add our callback which will be called for every found link
 rules = [
   Rule(SgmlLinkExtractor(), follow=True)
 ]</pre>
<p>Start Scrapy to test it&#8217;s crawling properly. Run the following the top level directoty:</p>
<pre>scrapy crawl mycrawler</pre>
<p>You should see output like:</p>
<pre>2011-03-08 15:25:52+0200 [scrapy] INFO: Scrapy 0.12.0.2538 started (bot: myscraper)
2011-03-08 15:25:52+0200 [scrapy] DEBUG: Enabled extensions: TelnetConsole, SpiderContext, WebService, CoreStats, MemoryUsage, CloseSpider
2011-03-08 15:25:52+0200 [scrapy] DEBUG: Enabled scheduler middlewares: DuplicatesFilterMiddleware</pre>
<p>You can hit CTRL+C to interrupt scrapy.</p>
<p>Then let&#8217;s enhance the spider a bit to search for a blacklisted tags, with optional whitelisting in myscraper/spiders/spiders.py. We use also <a href="http://pybrary.net/pyPdf/">pyPdf</a> library to crawl inside PDF files:</p>
<pre>"""

        A sample crawler for seeking a text on sites.

"""

import StringIO

from functools import partial

from scrapy.http import Request

from scrapy.spider import BaseSpider
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor

from scrapy.item import Item

def find_all_substrings(string, sub):
    """

http://code.activestate.com/recipes/499314-find-all-indices-of-a-substring-in-a-given-string/

    """
    import re
    starts = [match.start() for match in re.finditer(re.escape(sub), string)]
    return starts

class MySpider(CrawlSpider):
    """ Crawl through web sites you specify """

    name = "mycrawler"

    # Stay within these domains when crawling
    allowed_domains = ["www.mysite.com", "www.mysite2.com", "intranet.mysite.com"]

    start_urls = [
        "http://www.mysite.com/",
        "http://www.mysite2.com/",
        "http://intranet.mysite.com/"
    ]

    # Add our callback which will be called for every found link
    rules = [
        Rule(SgmlLinkExtractor(), follow=True, callback="check_violations")
    ]

    # How many pages crawled? XXX: Was not sure if CrawlSpider is a singleton class
    crawl_count = 0

    # How many text matches we have found
    violations = 0

    def get_pdf_text(self, response):
        """ Peek inside PDF to check possible violations.

        @return: PDF content as searcable plain-text string
        """

        try:
                from pyPdf import PdfFileReader
        except ImportError:
                print "Needed: easy_install pyPdf"
                raise 

        stream = StringIO.StringIO(response.body)
        reader = PdfFileReader(stream)

        text = u""

        if reader.getDocumentInfo().title:
                # Title is optional, may be None
                text += reader.getDocumentInfo().title

        for page in reader.pages:
                # XXX: Does handle unicode properly?
                text += page.extractText()

        return text                                      

    def check_violations(self, response):
        """ Check a server response page (file) for possible violations """

        # Do some user visible status reporting
        self.__class__.crawl_count += 1

        crawl_count = self.__class__.crawl_count
        if crawl_count % 100 == 0:
                # Print some progress output
                print "Crawled %d pages" % crawl_count

        # Entries which are not allowed to appear in content.
        # These are case-sensitive
        blacklist = ["meat", "ham" ]

        # Enteries which are allowed to appear. They are usually
        # non-human visible data, like CSS classes, and may not be interesting business wise
        exceptions_after = [ "meatball",
                             "hamming",
                             "hamburg"
                     ]

        # These are predencing string where our match is allowed
        exceptions_before = [
                "bushmeat",
                "honeybaked ham"
        ]

        url = response.url

        # Check response content type to identify what kind of payload this link target is
        ct = response.headers.get("content-type", "").lower()
        if "pdf" in ct:
                # Assume a PDF file
                data = self.get_pdf_text(response)
        else:
                # Assume it's HTML
                data = response.body

        # Go through our search goals to identify any "bad" text on the page
        for tag in blacklist:

                substrings = find_all_substrings(data, tag)

                # Check entries against the exception list for "allowed" special cases
                for pos in substrings:
                        ok = False
                        for exception in exceptions_after:
                                sample = data[pos:pos+len(exception)]
                                if sample == exception:
                                        #print "Was whitelisted special case:" + sample
                                        ok = True
                                        break

                        for exception in exceptions_before:
                                sample = data[pos - len(exception) + len(tag): pos+len(tag) ]
                                #print "For %s got sample %s" % (exception, sample)
                                if sample == exception:
                                        #print "Was whitelisted special case:" + sample
                                        ok = True
                                        break
                        if not ok:
                                self.__class__.violations += 1
                                print "Violation number %d" % self.__class__.violations
                                print "URL %s" % url
                                print "Violating text:" + tag
                                print "Position:" + str(pos)
                                piece = data[pos-40:pos+40].encode("utf-8")
                                print "Sample text around position:" + piece.replace("\n", " ")
                                print "------"

        # We are not actually storing any data, return dummy item
        return Item()

    def _requests_to_follow(self, response):

        if getattr(response, "encoding", None) != None:
                # Server does not set encoding for binary files
                # Do not try to follow links in
                # binary data, as this will break Scrapy
                return CrawlSpider._requests_to_follow(self, response)
        else:
                return []</pre>
<p>Let&#8217;s tune down logging output level, so we get only relevant data in the output. In <em>myscaper/settings.py</em> add:</p>
<pre>LOG_LEVEL="INFO"</pre>
<p>Now you can run the crawler and pipe the output to a text file:</p>
<pre>scrapy crawl mycrawler &gt; violations.txt</pre>
<p>More information</p>
<ul>
<li><a href="http://doc.scrapy.org/intro/install.html#intro-install-easy">Scrapy manual</a></li>
</ul>
<p class="signature">
 <a href="http://feeds.feedburner.com/OpenSourceHacker" rel="alternate" type="application/rss+xml"><img valign="middle" src="//www.feedburner.com/fb/images/pub/feed-icon16x16.png" alt="" style="border:0"/></a> <a href="http://feeds.feedburner.com/OpenSourceHacker" rel="alternate" type="application/rss+xml">Subscribe to RSS feed</a> <a href="http://twitter.com/moo9000"> <img valign="middle"  style="border:0" src="//opensourcehacker.com/wp-content/uploads/twitter-24.png"></a> <a href="http://twitter.com/moo9000">Follow me on Twitter</a> 
 <a href="https://www.facebook.com/pages/Open-Source-Hacker/181710458567630"> <img valign="middle"  style="border:0" src="//opensourcehacker.com/wp-content/uploads/facebook-24.png"></a> <a href="https://www.facebook.com/pages/Open-Source-Hacker/181710458567630">Follow me on Facebook</a> <a href="https://plus.google.com/103323677227728078543/"><img valign="middle"  style="border:0" src="//opensourcehacker.com/wp-content/uploads/googleplus.png"></a> <a href="https://plus.google.com/103323677227728078543/">Follow me Google+</a></p>
			</div><!-- .entry-content -->

	<footer class="entry-meta">
		This entry was posted in <a href="https://opensourcehacker.com/category/python/" rel="category tag">python</a>, <a href="https://opensourcehacker.com/category/technology/" rel="category tag">technology</a> and tagged <a href="https://opensourcehacker.com/tag/crawl/" rel="tag">crawl</a>, <a href="https://opensourcehacker.com/tag/find/" rel="tag">find</a>, <a href="https://opensourcehacker.com/tag/follow/" rel="tag">follow</a>, <a href="https://opensourcehacker.com/tag/full-text/" rel="tag">full text</a>, <a href="https://opensourcehacker.com/tag/full-text-search/" rel="tag">full text search</a>, <a href="https://opensourcehacker.com/tag/pdf/" rel="tag">pdf</a>, <a href="https://opensourcehacker.com/tag/pypdf/" rel="tag">pypdf</a>, <a href="https://opensourcehacker.com/tag/python/" rel="tag">python</a>, <a href="https://opensourcehacker.com/tag/scrape/" rel="tag">scrape</a>, <a href="https://opensourcehacker.com/tag/scrapy/" rel="tag">scrapy</a>, <a href="https://opensourcehacker.com/tag/search/" rel="tag">search</a>, <a href="https://opensourcehacker.com/tag/trademark/" rel="tag">trademark</a>, <a href="https://opensourcehacker.com/tag/user-policy/" rel="tag">user policy</a>, <a href="https://opensourcehacker.com/tag/violation/" rel="tag">violation</a>, <a href="https://opensourcehacker.com/tag/web-crawler/" rel="tag">web crawler</a> by <a href="https://opensourcehacker.com/author/moo/">Mikko Ohtamaa</a>. Bookmark the <a href="https://opensourcehacker.com/2011/03/08/installing-and-using-scrapy-web-crawler-to-search-text-on-multiple-sites/" title="Permalink to Installing and using Scrapy web crawler to search text on multiple sites" rel="bookmark">permalink</a>.		
			</footer><!-- .entry-meta -->
</article><!-- #post-1073 -->

						<div id="comments">
	
	
			<h2 id="comments-title">
			One thought on &ldquo;<span>Installing and using Scrapy web crawler to search text on multiple sites</span>&rdquo;		</h2>

		
		<ol class="commentlist">
				<li class="comment even thread-even depth-1" id="li-comment-15990">
		<article id="comment-15990" class="comment">
			<footer class="comment-meta">
				<div class="comment-author vcard">
					<img alt='' src='//secure.gravatar.com/avatar/e933772d926209ef3ead1d869d365e09?s=68&amp;d=monsterid&amp;r=G' class='avatar avatar-68 photo' height='68' width='68' /><span class="fn"><a href='http://hanumesh13.wordpress.com' rel='external nofollow' class='url'>hanumesh</a></span> on <a href="https://opensourcehacker.com/2011/03/08/installing-and-using-scrapy-web-crawler-to-search-text-on-multiple-sites/comment-page-1/#comment-15990"><time pubdate datetime="2013-05-28T09:30:06+00:00">2013-05-28 at 09:30</time></a> <span class="says">said:</span>
									</div><!-- .comment-author .vcard -->

				
			</footer>

			<div class="comment-content"><p>hi..<br />
 i am getting error at crawl_conut =0<br />
error:<br />
 crawl_count = 0<br />
    ^<br />
IndentationError: unexpected indent</p>
<p>code :<br />
    # How many pages crawled? XXX: Was not sure if CrawlSpider is a singleton class<br />
    crawl_count = 0</p>
<p> i am i missing someting .. i need to know what should i code in items.py</p>
</div>

			<div class="reply">
							</div><!-- .reply -->
		</article><!-- #comment-## -->

	</li><!-- #comment-## -->
		</ol>

		
	
									<div id="respond" class="comment-respond">
				<h3 id="reply-title" class="comment-reply-title">Leave a Reply <small><a rel="nofollow" id="cancel-comment-reply-link" href="/2011/03/08/installing-and-using-scrapy-web-crawler-to-search-text-on-multiple-sites/#respond" style="display:none;">Cancel reply</a></small></h3>
									<form action="//opensourcehacker.com/wp-comments-post.php" method="post" id="commentform" class="comment-form">
																			<p class="comment-notes"><span id="email-notes">Your email address will not be published.</span> Required fields are marked <span class="required">*</span></p>							<p class="comment-form-author"><label for="author">Name <span class="required">*</span></label> <input id="author" name="author" type="text" value="" size="30" aria-required='true' /></p>
<p class="comment-form-email"><label for="email">Email <span class="required">*</span></label> <input id="email" name="email" type="text" value="" size="30" aria-describedby="email-notes" aria-required='true' /></p>
<p class="comment-form-url"><label for="url">Website</label> <input id="url" name="url" type="text" value="" size="30" /></p>
												<p class="comment-form-comment"><label for="comment">Comment</label> <textarea id="comment" name="comment" cols="45" rows="8" aria-describedby="form-allowed-tags" aria-required="true"></textarea></p>						<p class="form-allowed-tags" id="form-allowed-tags">You may use these <abbr title="HyperText Markup Language">HTML</abbr> tags and attributes:  <code>&lt;a href=&quot;&quot; title=&quot;&quot;&gt; &lt;abbr title=&quot;&quot;&gt; &lt;acronym title=&quot;&quot;&gt; &lt;b&gt; &lt;blockquote cite=&quot;&quot;&gt; &lt;cite&gt; &lt;code&gt; &lt;del datetime=&quot;&quot;&gt; &lt;em&gt; &lt;i&gt; &lt;q cite=&quot;&quot;&gt; &lt;strike&gt; &lt;strong&gt; </code></p>						<p class="form-submit">
							<input name="submit" type="submit" id="submit" class="submit" value="Post Comment" />
							<input type='hidden' name='comment_post_ID' value='1073' id='comment_post_ID' />
<input type='hidden' name='comment_parent' id='comment_parent' value='0' />
						</p>
						<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="768a6ce847" /></p><p style="display:none;"><input type="text" name="nxts" value="1427385734" /><input type="text" name="nxts_signed" value="c2fa0e3cb9409ad9b4c3065e9e26ff6f80917a1c" /><input type="text" name="d7cd6414ce8bf948533fc331dac6bae9" value="c5b3c574fc6c43d1e4b2e6d584e19269" /><input type="text" name="adfbd8874a1bc8421f1949b762a7ae03" value="" /></p>

	<p style="clear: both;" class="subscribe-to-comments">
	<input type="checkbox" name="subscribe" id="subscribe" value="subscribe" style="width: auto;" />
	<label for="subscribe">Notify me of followup comments via e-mail</label>
	</p>


					</form>
							</div><!-- #respond -->
			
</div><!-- #comments -->

				
			</div><!-- #content -->
		</div><!-- #primary -->



	</div><!-- #main -->

	<footer id="colophon" role="contentinfo">

			

			<div id="site-generator">
				open source hacker: get your dose of Linux, Ubuntu, Python, Javascript, HTML5 and other cool and free technology. 
			</div>

 			<div style="display: none">
				<a rel="author" href="https://plus.google.com/103323677227728078543">Google+ profile</a>
 			</div>
	</footer><!-- #colophon -->
</div><!-- #page -->


<!-- tracker added by Ultimate Google Analytics plugin v1.5.3: http://www.oratransplant.nl/uga -->
<script src="//www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-25261116-1";
urchinTracker();
</script>
<img src="//opensourcehacker.com/wp-content/plugins/cookies-for-comments/css.php?k=f0702d90d271949453d211b6efabce7a&amp;o=i&amp;t=1843451892" width='1' height='1' />

</body>
</html>